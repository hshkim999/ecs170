# ecs170

When ASL interpreter Justina Miles performed at Rihannaâ€™s SuperBowl performance earlier this year, her expressive interpretations were considered a prime example of making media more accessible to individuals that are deaf or hard-of-hearing. When selecting a project concept, we were similarly inspired by the importance of ASL in making everyday activities more accessible to all groups of people. Our goal was to create a model to detect and translate American Sign Language (ASL) fingerspelling into text predictions, which has significant implications for live captioning and media accessibility.  Throughout the project and conversations with peers, we noticed the lack of undergraduate-level knowledge about Transformers, a model that is heavily used in automated speech recognition (ASR) and text generation models and features an encoding-decoding format. Aiming to make our project more accessible to peers, we modified our project scope to emphasize making Transformer-based predictive models more accessible to undergraduate students. We first conducted a literature review of Transformers, and used our learnings to successfully replicate an ASL-to-text predictive model. Then, we added extensive documentation on the model to serve as a resource for others to better understand how they can implement basic Transformers in an ASL-to-text setting.  Finally, we conducted an analysis of multiple prize-winning Transformer-based ASL-to-text models to determine how ASL-to-text models can be further improved. By conducting a thorough literature review of Transformers and applying our learning to successfully implement and document a Transformer-based prediction model, our team was able to develop a technical understanding of Transformers while also gaining understanding of modern implementations of Transformers and sharing that knowledge with the class.  
